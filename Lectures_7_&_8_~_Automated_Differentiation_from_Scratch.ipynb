{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNm8sBsNsWyk7HYnsdV7c5x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LunaEyad/Deep-Learning-Foundations/blob/main/Lectures_7_%26_8_~_Automated_Differentiation_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vanilla python Closed Form Solution\n"
      ],
      "metadata": {
        "id": "iRNrkC_KnBi3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htwz0t06dxbT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e94b44f-1b95-48c5-d058-434cb3a131ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clsoed form: gradient for x_p = -0.3277744397151291, gradient for y_p = -0.336282174741702\n",
            "Clsoed form: loss = 0.4430528244756474\n"
          ]
        }
      ],
      "source": [
        "#generate the points\n",
        "from random import Random\n",
        "from math import ceil , sqrt\n",
        "\n",
        "def generate_pnts(n=1000):\n",
        "  seed=5\n",
        "  random_num=Random(x=seed)\n",
        "  return(\n",
        "      [random_num.uniform(a=0,b=1)for _ in range(n)],\n",
        "      [random_num.uniform(a=0,b=1)for _ in range(n)]\n",
        "  )\n",
        "\n",
        "\n",
        "#clac grad\n",
        "def clac_grad(x_p ,y_p , batch_x , batch_y):\n",
        "\n",
        "  sum_x , sum_y= 0 ,0\n",
        "  n=len(batch_x)\n",
        "  for x_i, y_i in zip(batch_x ,batch_y):\n",
        "    inv_sqrt = ((x_i - x_p) ** 2 + (y_i - y_p) ** 2) ** (-0.5)\n",
        "    sum_x += inv_sqrt * (x_i - x_p)\n",
        "    sum_y += inv_sqrt * (y_i - y_p)\n",
        "  return -sum_x/n, -sum_y/n\n",
        "\n",
        "\n",
        "#loss fn\n",
        "def loss_fn(x_p ,y_p , batch_x , batch_y):\n",
        " \n",
        "  return (1/len(batch_x))* sum([sqrt((x_i-x_p)**2+(y_i-y_p)**2) for x_i , y_i in zip(batch_x, batch_y)])\n",
        "\n",
        "data_x, data_y = generate_pnts(n=1000)\n",
        "x_p , y_p = 0.3 , 0.3\n",
        "grad_x , grad_y =clac_grad(x_p, y_p, data_x ,data_y)\n",
        "curr_loss = loss_fn(x_p, y_p, data_x ,data_y)\n",
        "\n",
        "print(f\"Clsoed form: gradient for x_p = {grad_x}, gradient for y_p = {grad_y}\")\n",
        "print(f\"Clsoed form: loss = {curr_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytorch auto-grad engine"
      ],
      "metadata": {
        "id": "DisUVe8onb-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "pnt =torch.tensor([0.3 , 0.3])\n",
        "pnt.requires_grad = True\n",
        "pnt.retain_grad()\n",
        "data_x , data_y =generate_pnts(n=1000)\n",
        "data=torch.tensor([data_x , data_y])\n",
        "data=data.t()\n",
        "\n",
        "for i in range(2):\n",
        "  loss_torch=torch.mean(torch.sqrt(((data-pnt)**2).sum(dim=1)))\n",
        "  print(f\"torch loss: {loss_torch}\")\n",
        "  loss_torch.backward()\n",
        "  print(f\"torch auto gradient: {pnt.grad.data}\")\n",
        "  pnt.grad.zero_()\n",
        "  print(f\"gradient after zeroing : {pnt.grad.data}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLAqfiiZnBUz",
        "outputId": "48bb4f10-6d83-428c-cec0-c2dd034c9171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch loss: 0.44305282831192017\n",
            "torch auto gradient: tensor([-0.3278, -0.3363])\n",
            "gradient after zeroing : tensor([0., 0.])\n",
            "torch loss: 0.44305282831192017\n",
            "torch auto gradient: tensor([-0.3278, -0.3363])\n",
            "gradient after zeroing : tensor([0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building our own Auto-Grad from scratch"
      ],
      "metadata": {
        "id": "d1WkzFRs8jX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class comp_node:\n",
        "  def __init__(self , val , children=[], op=\"assign\"):\n",
        "    self.val=val\n",
        "    self.children=children\n",
        "    self.op=op\n",
        "    self.grad=0\n",
        "    self.backward_prop= lambda: None\n",
        "\n",
        "  def to_comp_node(self, obj):\n",
        "    if not isinstance(obj, comp_node):\n",
        "     return comp_node(val = obj)\n",
        "    else:\n",
        "      return obj\n",
        "\n",
        "#node1(self) - 3(other)\n",
        "  def __sub__(self,other): \n",
        "    other=self.to_comp_node(other)\n",
        "    out= comp_node(self.val-other.val , children=[self , other] , op=\"sub\")\n",
        "    def __backward_prop():\n",
        "      self.grad+=out.grad*1\n",
        "      other.grad+=out.grad*-1\n",
        "\n",
        "    out.backward_prop=__backward_prop\n",
        "\n",
        "    return out\n",
        "\n",
        "#3(other)-node1(self)\n",
        "  def __rsub__(self,other): \n",
        "    other=self.to_comp_node(other)\n",
        "    return other - self\n",
        "\n",
        "  def __pow__(self, exponenet):\n",
        "    if not isinstance (exponenet , (int ,float)):\n",
        "      raise ValueError (\"unsupported type\")\n",
        "    out= comp_node(val=self.val** exponenet , children=[self], op=f\"pow of {exponenet}\")\n",
        "\n",
        "    def __backward_prop():\n",
        "      self.grad+=out.grad* (exponenet*self.val**(exponenet-1))\n",
        "\n",
        "    out.backward_prop=__backward_prop\n",
        "    return out\n",
        "  \n",
        "\n",
        "\n",
        "  def __add__(self,other): \n",
        "    other= self.to_comp_node(other)\n",
        "    out= comp_node(self.val + other.val , children=[self , other] , op=\"add\")\n",
        "    def __backward_prop():\n",
        "      self.grad+=out.grad*1\n",
        "      other.grad+=out.grad*1\n",
        "\n",
        "    out.backward_prop=__backward_prop\n",
        "    return out\n",
        "\n",
        "  def __radd__(self,other): \n",
        "    other=self.to_comp_node(other)\n",
        "    return other + self\n",
        "\n",
        "  def __mul__(self,other): \n",
        "    other= self.to_comp_node(other)\n",
        "    out= comp_node(self.val * other.val , children=[self , other] , op=\"mul\")\n",
        "    def __backward_prop():\n",
        "       self.grad += out.grad * other.val\n",
        "       other.grad += out.grad * self.val\n",
        "    out.backward_prop = __backward_prop \n",
        "    return out\n",
        "\n",
        "  def __rmul__(self,other): \n",
        "    other=self.to_comp_node(other)\n",
        "    return other * self\n",
        "    \n",
        "  def __repr__(self): \n",
        "    return(f\"val={self.val:.4f}, children={len(self.children)}, op={self.op} , grad={self.grad}\") \n",
        "  \n"
      ],
      "metadata": {
        "id": "Jxx1mox-6VdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define two comp_node instances\n",
        "a = comp_node(5)\n",
        "b = comp_node(10)\n",
        "\n",
        "# Test subtraction operator\n",
        "c = a - b\n",
        "assert c.val == -5\n",
        "\n",
        "# Test reverse subtraction operator\n",
        "d = 20 - b\n",
        "assert d.val == 10\n",
        "\n",
        "# Test power operator\n",
        "i = a ** 2\n",
        "assert i.val == 25\n",
        "\n",
        "\n",
        "# Test addition operator\n",
        "c = a + b\n",
        "assert c.val == 15\n",
        "\n",
        "# Test reverse addition operator\n",
        "d = 20 + b\n",
        "assert d.val == 30\n",
        "\n",
        "\n",
        "# Test mult operator\n",
        "c = a * b\n",
        "assert c.val == 50\n",
        "\n",
        "# Test reverse mult operator\n",
        "d = 20 * b\n",
        "assert d.val == 200"
      ],
      "metadata": {
        "id": "vFXs-xaagQs8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_x, data_y = generate_pnts(n=1)\n",
        "print(data_x, data_y)\n",
        "x_p , y_p = comp_node(val=0.3) , comp_node(val=0.3)\n",
        "\n",
        "\n",
        "def loss_graph(x_p, y_p, data_x, data_y):\n",
        "  I_x , I_y = x_p - data_x , y_p -data_y\n",
        "  G_x , G_y=  I_x **2 , I_y**2\n",
        "  M = G_x + G_y\n",
        "  l=M**0.5\n",
        "  return  l , [l , M ,G_x , G_y ,I_x , I_y , x_p, y_p]\n",
        "\n",
        "l , rev_top_sort =loss_graph(x_p, y_p, data_x[0], data_y[0])  \n",
        "rev_top_sort[0].grad=1\n",
        "\n",
        "for i , node in enumerate(rev_top_sort):\n",
        "\n",
        "  node.backward_prop()\n",
        "  print(i , node)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxoWnyUUHuWw",
        "outputId": "eb3aae7c-5e8f-4049-aa13-bd2b420208a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.6229016948897019] [0.7417869892607294]\n",
            "0 val=0.5472, children=1, op=pow of 0.5 , grad=1\n",
            "1 val=0.2994, children=2, op=add , grad=0.9137222319490423\n",
            "2 val=0.1043, children=1, op=pow of 2 , grad=0.9137222319490423\n",
            "3 val=0.1952, children=1, op=pow of 2 , grad=0.9137222319490423\n",
            "4 val=-0.3229, children=2, op=sub , grad=-0.5900849147094943\n",
            "5 val=-0.4418, children=2, op=sub , grad=-0.8073411877467226\n",
            "6 val=0.3000, children=0, op=assign , grad=-0.5900849147094943\n",
            "7 val=0.3000, children=0, op=assign , grad=-0.8073411877467226\n"
          ]
        }
      ]
    }
  ]
}